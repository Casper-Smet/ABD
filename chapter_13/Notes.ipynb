{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "previous-uncertainty",
   "metadata": {},
   "source": [
    "# 13. Quantifying uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-blast",
   "metadata": {},
   "source": [
    "## 13.1 Acting under uncertainty\n",
    "\n",
    "Agents need to handle **uncertainty**, whether due to partial observability, nondeterminism, or the combination of the two.\n",
    "   \n",
    "Agents can solve problems by handling uncertainty by keeping track of a **belief state** (a representation of the set of all possible worlds states that it might be in and generating a contingency plan that handles every possible eventuality that its sensors may report during execution. But this has some drawbacks:\n",
    "- agent must consider **every** possible explanation for the observations, no matter how unlikely\n",
    "- a contingency plan can grow very large\n",
    "- sometimes there is no plan that is guaranteed to achieve the goal, yet the agent must act\n",
    "  \n",
    "This leads to a **qualification problem** (Specifying all the exceptions for which an action could fail (none of the conditions can be deduced for sure, so the plan's success cannot be inferred))\n",
    "\n",
    "The **rational decision** depends on both the relative importance of various goals and the likelihood that, and degree to which, they will be achieved.\n",
    "\n",
    "### Summarizing uncertainty\n",
    "\n",
    "Let's consider an example for uncertain reasoning:  \n",
    "$Toothache \\rightarrow Cavity$  \n",
    "Not all patients with toothache have cavities, if we switch it:  \n",
    "$Cavity \\rightarrow Toothache$  \n",
    "But this rule is not right either; not all cavities cause pain.  \n",
    "  \n",
    "The connection between toothaches and cavities is just not a logical consequence in either direction. The only way to fix the rule is to make it logically exhaustive: to add all qualifications required for a cavity to cause a toothache. Trying to use logic to cope with a judgemental domain fails for 3 reasons:\n",
    "- **Laziness** - Too much work to list the complete set\n",
    "- **Theoretical ignorance** - No complete theory for the domain\n",
    "- **Practical ignorance** - Even if the rules are known, we still might be uncertain  \n",
    "\n",
    "The agent's knowledge can at best provide only a **degree of belief** in the relevant sentences. The main tool for dealing with degrees of belief is **probability theory**.  \n",
    "\n",
    "Probability provides a way of **summarizing** the uncertainty that comes from our laziness and ignorance.  \n",
    "\n",
    "There is no uncertainty in the actual world: The patient either has a cavity or doesn't. Probability statements are made with respect to a knowledge state, not with respect to the real word. We say: *The probability that a patient has a cavity, **given that she has a toothache** is 0.8*.\n",
    "\n",
    "### Uncertainty and rational decisions\n",
    "\n",
    "To make a choice, an agent must have preferences between the different possible outcomes of the various plans. We use utility theory to represent and reason with preferences. Utility theory says that every state has a degree of usefulness (or utility) to an agent and that the agent will prefer states with higher utility.  \n",
    "  \n",
    "The ulitity state is relative to an agent. For example, the utility of a state in which White has checkmated Black in a game of chess is obviously high for the player playing White, but low for the player playing Black.  \n",
    "\n",
    "**decision theory** calls for preferences, as expressed by utilities, combined with probabilities.  \n",
    "The fundamental idea of decision theory is that *an agent is rational if and only if it chooses the action that yields the highest expected utility, averaged over all the possible outcomes of the action*. This is also called the principle of **maximum expected utility** (MEU).\n",
    "\n",
    "\n",
    "## 13.2 Basic probability notation\n",
    "\n",
    "### Probabilities\n",
    "\n",
    "In probability theory, the set of all possible worlds is called the **sample space**. e.g. for the roll of two dices, there are 36 possible worlds to consider. A fully specified **probability model** associates a numerical probability $P(\\omega)$ with each possible world.  \n",
    "The basic axioms of probability theory say that every possible world has a probability between 0 and 1 and that the total probability of the set of possible worlds is 1.  \n",
    "  \n",
    "When we are interested in a certain set (in probability theory called **events**), we can describe them by **propositions** in formal language. The probability associated with a proposition is defined to be the sum of the probabilities of the worlds in which it holds.  \n",
    "For Example, when we roll a fair dice the probability of $P(Total = 11)$ is $1/36 + 1/36 = 1/18$. These probabilities (such as $P(Total = 5)$ and $P(doubles)$ are called **unconditional** or **prior** probabilites; they refer to degrees of belief in propositions *in the abscence of any other information*.  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-detection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
